Namespace(arch='resnet50', batch_size=32, bottleneck_dim=256, data='Office31_v2', epochs=20, iters_per_epoch=1000, log='/home/sean/CS769/project/Transfer-Learning-Library/sandbox/logs/dann/Office31_v2/v4', lr=0.01, lr_decay=0.75, lr_gamma=0.001, momentum=0.9, no_hflip=False, no_pool=False, norm_mean=(0.485, 0.456, 0.406), norm_std=(0.229, 0.224, 0.225), per_class_eval=False, phase='test', print_freq=100, resize_size=224, root='/home/sean/CS769/project/Transfer-Learning-Library/data/office31_v2', scratch=False, seed=1, source_test='A_test', source_train='A_train', source_val='A_val', target_test='W_test', target_train='W_train', target_val='W_val', trade_off=0.0, train_resizing='default', val_resizing='default', version='v4', weight_decay=0.001, workers=2)
sandbox/dann_v2.py:46: UserWarning: You have chosen to seed training. This will turn on the CUDNN deterministic setting, which can slow down your training considerably! You may see unexpected behavior when restarting from checkpoints.
  warnings.warn('You have chosen to seed training. '
train_transform:  Compose(
    Compose(
    ResizeImage(size=(256, 256))
    RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=PIL.Image.BILINEAR)
)
    RandomHorizontalFlip(p=0.5)
    ToTensor()
    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
)
val_transform:  Compose(
    Compose(
    ResizeImage(size=(256, 256))
    CenterCrop(size=(224, 224))
)
    ToTensor()
    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
)
=> using model 'resnet50'
Top-1 accuracy on source_train_loader: 98.49759615384616
Top-1 accuracy on target_train_loader: 49.107142857142854
Top-1 accuracy on source_val_loader: 87.38898753950481
Top-1 accuracy on target_val_loader: 57.86163558000289
Top-1 accuracy on source_test_loader: 88.29787234042553
Top-1 accuracy on target_test_loader: 55.97484283927102
